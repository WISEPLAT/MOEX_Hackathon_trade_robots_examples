{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c828fe",
   "metadata": {},
   "source": [
    "# Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e73a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import pandas as pd\n",
    "from matplotlib.dates import MONDAY, DateFormatter, DayLocator, WeekdayLocator\n",
    "\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a677b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from moexalgo import Market, Ticker\n",
    "import io\n",
    "from PIL import Image\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7c9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ad5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b359da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2915445",
   "metadata": {},
   "source": [
    "# Импортируем модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10132406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8899a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c517873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модули генерации датасета\n",
    "from date_filter import date_filter #Фильтрация данных по датам date_filter(quotes, filter_data_timezone, filter_data_start, filter_data_end)\n",
    "from show_quotes import show_quotes #Смотрим исходные данные show_quotes(quotes)\n",
    "from get_extrems import get_extrems #Получаем экстремумы get_extrems(dataset, delete_not_marking_data, count_points = 6)\n",
    "from show_quotes_with_trends import show_quotes_with_trends #Просмотр результатов разметки show_quotes_with_trends(quotes_with_extrems, show = False)\n",
    "from quotes_with_Y import quotes_with_Y#Разметка Y quotes_with_Y(quotes_with_extrems, extr_bar_count, Y_shift)\n",
    "from get_indicators import get_indicators #Получение индикаторов для котировок get_indicators(df, prefix = ':1d')\n",
    "from get_stoch_indicators import get_stoch_indicators#Обработка стохастика над индикаторами get_stoch_indicators(df, prefix = ':1d')\n",
    "from get_stoch_logic_data import get_stoch_logic_data#Генерация логического датасета над датасетом стохастика get_stoch_logic_data(df, prefix = ':1d')\n",
    "from norm_num_df import norm_num_df# Генерация нормализованного числового датасета norm_num_df(df, prefix = ':1d')\n",
    "from waves_dataset import waves_dataset#Генерация датасета по экстремумам waves_dataset(df, prefix = ':1d')\n",
    "from logic_dataset import logic_dataset#Генерация датасета на основании логических конструкций logic_dataset(df, prefix = ':1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2d100",
   "metadata": {},
   "source": [
    "# Параметры генерируемого датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7caa753",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params_from_config_file = True #Загрузка параметров из файла\n",
    "load_params_from_command_line = False #Загрузка параметров из командной строки\n",
    "args = None\n",
    "\n",
    "try:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    _ = parser.add_argument('--config_file', dest='config_file', action='store_true', help='Load config from file')\n",
    "    _ = parser.add_argument('--config_path', help='Path to config file: /app/cfg.json')\n",
    "    _ = parser.add_argument('--cmd_config', dest='cmd_config', action='store_true', help='Load config from cmd line')\n",
    "    _ = parser.add_argument('--task_id')\n",
    "    _ = parser.add_argument('--timeframe')\n",
    "    _ = parser.add_argument('--start_date')\n",
    "    _ = parser.add_argument('--end_date')\n",
    "    _ = parser.add_argument('--count_points')\n",
    "    _ = parser.add_argument('--extr_bar_count')\n",
    "    _ = parser.add_argument('--size_df')\n",
    "    _ = parser.add_argument('--max_unmark')\n",
    "    _ = parser.add_argument('--data_path')\n",
    "    _ = parser.add_argument('--respos_url')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if args.config_file:\n",
    "        load_params_from_config_file = True\n",
    "        load_params_from_command_line = False\n",
    "    \n",
    "    if args.cmd_config:\n",
    "            load_params_from_config_file = False\n",
    "            load_params_from_command_line = True\n",
    "except:\n",
    "    print(\"Ошибка парсинга параметров из командной строки\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345acd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_params_from_config_file:\n",
    "    #Если есть параметры командной строки\n",
    "    if args:\n",
    "        #Если указан путь к конфигу\n",
    "        if args.config_path:\n",
    "            with open(config_path, 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "        else:\n",
    "            with open('app/configs/1D/data_gen.json', 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "\n",
    "    # parse file\n",
    "    config = json.loads(temp_data)\n",
    "    \n",
    "    task_id = str(config['task_id'])\n",
    "    interval = config['timeframe']\n",
    "    start_date = config['start_date'] #Начальная дата датасета\n",
    "    end_date = config['end_date'] #Конечная дата датасета\n",
    "    count_points = config['count_points'] #Параметр разметки экстремумов\n",
    "    #Сколько размечаем баров начиная с точки экстремума\n",
    "    extr_bar_count = config['extr_bar_count']\n",
    "    #Ограничения размера файла в Гигабайтах\n",
    "    size_df = config['size_df']\n",
    "    #Максимальное количество конечных баров волны в %, которые не размечаем\n",
    "    max_unmark = config['max_unmark']\n",
    "    #Путь для сохранения генерируемых данных\n",
    "    data_path = config['data_path'] #Путь должен быть без чёрточки в конце\n",
    "    if config['respos_url']:\n",
    "        respos_url = config['respos_url']\n",
    "    else:\n",
    "        respos_url = '127.0.0.1:8080'\n",
    "    \n",
    "if load_params_from_command_line:\n",
    "    task_id = str(args.task_id)\n",
    "    interval = str(args.timeframe)\n",
    "    start_date = str(args.start_date)\n",
    "    end_date = str(args.end_date) \n",
    "    count_points = int(args.count_points)\n",
    "    extr_bar_count = int(args.extr_bar_count) \n",
    "    size_df = float(args.size_df) \n",
    "    max_unmark = float(args.max_unmark) \n",
    "    data_path = str(args.data_path) \n",
    "    if args.respos_url:\n",
    "        respos_url = str(args.respos_url).replace(\"\\\\\",\"\").replace(\"/\",\"\").replace(']',\"\").replace('[',\"\").replace('\"',\"\").replace(\"'\",\"\")\n",
    "    else:\n",
    "        respos_url = '127.0.0.1:8080'\n",
    "\n",
    "Y_shift = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee87a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835a7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d696ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Смещение категориальных признаков разметки\n",
    "Y_shift = 1\n",
    "\n",
    "#Флаг необходимости формирования трендовых признаков\n",
    "lag_flag = True\n",
    "\n",
    "#Число баров, которые мы кладём в датасет для формирования признаков трендовости\n",
    "#Число включает начальный бар без лага, то есть из 6: 1 - начальный + 5 лаговые\n",
    "#lag_count = 6 #(default)\n",
    "lag_count = 0\n",
    "\n",
    "#Дописывать данные (False) или заново записать датасет (True)\n",
    "new_df = False\n",
    "\n",
    "#Флаг наличия ограничений генерируемых датасетов по размеру\n",
    "size_flag = True\n",
    "\n",
    "#Флаг необходимости удаления не размеченных данных\n",
    "delete_not_marking_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20bbd42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f1d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_to_png(graph):\n",
    "    buffer = io.BytesIO()\n",
    "    graph.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_png = buffer.getvalue()\n",
    "    buffer.close()\n",
    "    graphic = base64.b64encode(image_png)\n",
    "    graphic = graphic.decode('utf-8')\n",
    "    graph.close()\n",
    "\n",
    "    return graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c97a086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main (ticker):\n",
    "    \n",
    "    #КОТИРОВКИ!\n",
    "    quotes_temp = Ticker(ticker)\n",
    "    # Свечи по акциям за период\n",
    "    quotes_1d = quotes_temp.candles(date = start_date, till_date = end_date, period=interval)\n",
    "    #quotes_1d.head()\n",
    "    quotes_1d = pd.DataFrame(quotes_1d)\n",
    "    \n",
    "    quotes_1d.rename(\n",
    "        columns = {\n",
    "            'begin' : 'Datetime',\n",
    "            'open' : 'Open',\n",
    "            'close' : 'Close',\n",
    "            'high' : 'High',\n",
    "            'low' : 'Low',\n",
    "            'volume' : 'Volume'\n",
    "        }, inplace = True\n",
    "    )\n",
    "    quotes_1d.index = quotes_1d['Datetime']\n",
    "    quotes_1d.sort_index(ascending=True, inplace = True)\n",
    "\n",
    "    #Получаем экстремумы по дневному графику\n",
    "    print('Получаем экстремумы по дневному графику')\n",
    "    quotes_1d_with_extrems = get_extrems(quotes_1d, delete_not_marking_data, count_points = count_points)\n",
    "\n",
    "    #Размечаем Y по дневному графику\n",
    "    quotes_1d_with_Y = quotes_with_Y(quotes_1d_with_extrems, extr_bar_count, Y_shift, max_unmark = max_unmark)\n",
    "\n",
    "    #Очищаем не размеченные данные\n",
    "    quotes_1d_with_Y = quotes_1d_with_Y.dropna(subset = ['Y'])\n",
    "\n",
    "    #Получаем данные индикаторов котировок дневного датафрейма\n",
    "    quotes_1d_indicators = get_indicators(quotes_1d_with_Y, prefix = ':5m')\n",
    "\n",
    "    #Получаем stoch датасет для котировок дневного таймфрейма\n",
    "    stoch_quotes_1d_dataset = get_stoch_indicators(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Получаем датасет логики над стохастиком для котировок дневного таймфрейма\n",
    "    stoch_logic_quotes_1d_dataset = get_stoch_logic_data(stoch_quotes_1d_dataset, prefix = ':5m')\n",
    "    \n",
    "    #Получаем нормализованный числовой датасет для котировок дневного таймфрейма\n",
    "    norm_num_dataset_quotes_1d = norm_num_df(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Свечной анализ\n",
    "    cdl_dataset_quotes_1d = quotes_1d.ta.cdl_pattern(name=\"all\")\n",
    "\n",
    "    #Датасет волн\n",
    "    waves_dataset_quotes_1d =  waves_dataset(quotes_1d_indicators, prefix = ':5m')\n",
    "\n",
    "    #Логический датасет\n",
    "    logic_dataset_quotes_1d =  logic_dataset(quotes_1d_indicators, prefix = ':5m')\n",
    "    \n",
    "    #Собираем датасеты\n",
    "    num_logic_df = pd.DataFrame()\n",
    "    \n",
    "    #Формируем индекс по древным котировкам\n",
    "    num_logic_df.index = quotes_1d.index\n",
    "    \n",
    "    #Инициализируем поля\n",
    "    num_logic_df['Close'] = quotes_1d_with_Y['Close']\n",
    "    num_logic_df['Y'] = quotes_1d_with_Y['Y']\n",
    "    \n",
    "    \n",
    "    #Джойним датасеты\n",
    "    num_logic_df = num_logic_df.join(norm_num_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')#Нормализованные дневные котировки\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(waves_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(cdl_dataset_quotes_1d, lsuffix='_left_num_qout_5m', rsuffix='_right_num_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(stoch_quotes_1d_dataset, lsuffix='_left_stoch_qout_5m', rsuffix='_right_stoch_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(stoch_logic_quotes_1d_dataset, lsuffix='_left_stoch_qout_5m', rsuffix='_right_stoch_qout_5m')\n",
    "    \n",
    "    num_logic_df = num_logic_df.join(logic_dataset_quotes_1d, lsuffix='_left_logic_qout_5m', rsuffix='_right_logic_qout_5m')\n",
    "    \n",
    "    \n",
    "    #Заполняем пустые ячейки предыдущими значениями\n",
    "    num_logic_df = num_logic_df.fillna(method=\"ffill\")\n",
    "     \n",
    "    #Добавляем лаги\n",
    "    #num_df\n",
    "    columns = num_logic_df.columns.values   \n",
    "    for col in columns:\n",
    "        if col not in ['Close', 'Y']:\n",
    "            try:\n",
    "                for i in range(1,lag_count):\n",
    "                    num_logic_df[col+'shift_'+str(i)] = num_logic_df[col].copy(deep = True).shift(i)\n",
    "            except:\n",
    "                #print(\"Ошибка добавления лага в колонке: \", col)\n",
    "                pass\n",
    "    \n",
    "    \n",
    "    #Чистим от пустых значений\n",
    "    num_logic_df = num_logic_df.dropna()\n",
    "    \n",
    "    #Конвертируем индексы\n",
    "    num_logic_df.index = num_logic_df.index.astype(int)\n",
    "    \n",
    "    #Разбиваем датасеты\n",
    "    num_logic_df_train, num_logic_df_test = train_test_split(num_logic_df, test_size=0.1, shuffle=False)\n",
    "    \n",
    "    #Записываем датасеты\n",
    "    print(\"Записываем датасеты: \", ticker)\n",
    "    #Проверяем на существование Если не существуют то записываем первый раз с заголовком\n",
    "    #Если существуют до дописываем без заголовка\n",
    "    if not os.path.exists(data_path+\"/num_logic_1d_1w_train.csv\"):\n",
    "        num_logic_df_train.to_csv(data_path+\"/num_logic_1d_1w_train.csv\")\n",
    "    else:\n",
    "        num_logic_df_train.to_csv(data_path+\"/num_logic_1d_1w_train.csv\", mode='a', header= False)\n",
    "    \n",
    "    if not os.path.exists(\"app/data/num_logic_1d_1w_test.csv\"):\n",
    "        num_logic_df_test.to_csv(data_path+\"/num_logic_1d_1w_test.csv\")\n",
    "    else:\n",
    "        num_logic_df_test.to_csv(data_path+\"/num_logic_1d_1w_test.csv\", mode='a', header= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754882e7",
   "metadata": {},
   "source": [
    "# Проверяем контрольную точку продолжения генерации датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52648857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_size():\n",
    "    #Проверяем наличие датасетов\n",
    "    folder = Path(data_path)\n",
    "    if os.path.exists(data_path):\n",
    "        if sum(1 for x in folder.iterdir()) > 0:\n",
    "            #Проверяем ограничения на размер файла\n",
    "            if size_flag:\n",
    "                size_arr = []\n",
    "                try:\n",
    "                    size_arr.append(os.path.getsize(data_path+\"/num_logic_1d_1w_train.csv\")/(1024*1024*1024))\n",
    "                except:\n",
    "                    size_arr.append(0)\n",
    "\n",
    "                max_size_df = max(size_arr)\n",
    "\n",
    "                if max_size_df > size_df:\n",
    "                    print (\"Достигнут предел по размеру датасетов\")\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "        else:\n",
    "            return True\n",
    "            \n",
    "    else:\n",
    "        os.mkdir(data_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aba69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверяем наличие датасетов\n",
    "folder = Path(data_path)\n",
    "if os.path.exists(data_path):\n",
    "    if sum(1 for x in folder.iterdir()) > 0:\n",
    "        #Проверяем ограничения на размер файла\n",
    "        if size_flag:\n",
    "            size_arr = []\n",
    "            try:\n",
    "                size_arr.append(os.path.getsize(data_path+\"/num_logic_1d_1w_train.csv\")/(1024*1024*1024))\n",
    "            except:\n",
    "                size_arr.append(0)\n",
    "            \n",
    "            max_size_df = max(size_arr)\n",
    "            \n",
    "            if max_size_df > size_df:\n",
    "                list_flag = False\n",
    "                print (\"Достигнут предел по размеру датасетов\")\n",
    "        \n",
    "        #Пытаемся загрузить последний тикер генерации датасета\n",
    "        try:\n",
    "            with open('save/last_ticker.pickle', 'rb') as f:\n",
    "                last_ticker = pickle.load(f)\n",
    "        except:\n",
    "            print(\"Отсутствуют данные сохранения\")\n",
    "else:\n",
    "    os.mkdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8be67",
   "metadata": {},
   "source": [
    "# Загружаем список для генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5bc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = Market('stocks')\n",
    "\n",
    "tickers_list_temp = stocks.tradestats(date='2023-10-10')\n",
    "tickers_list = pd.DataFrame(tickers_list_temp).rename(\n",
    "    columns = {\n",
    "        'secid': 'ticker'\n",
    "    }\n",
    ")\n",
    "tickers_list = tickers_list.groupby('ticker').agg(\n",
    "    {\n",
    "        'val': 'sum'\n",
    "    }\n",
    ").sort_values(by = ['val'], ascending = False).reset_index()['ticker'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588ab3f",
   "metadata": {},
   "source": [
    "# Генерируем датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711519d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBER\n",
      "Начало обработки нового тикера:  SBER\n",
      "Получаем экстремумы по дневному графику\n",
      "Общее число данных графика для обработки:  4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adimin\\AppData\\Local\\Temp\\ipykernel_44592\\3575762795.py:115: FutureWarning: The behavior of .astype from datetime64[ns] to int32 is deprecated. In a future version, this astype will return exactly the specified dtype instead of int64, and will raise if that conversion overflows.\n",
      "  num_logic_df.index = num_logic_df.index.astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Записываем датасеты:  SBER\n",
      "LKOH\n",
      "Начало обработки нового тикера:  LKOH\n",
      "Получаем экстремумы по дневному графику\n",
      "Общее число данных графика для обработки:  4223\n"
     ]
    }
   ],
   "source": [
    "if len(tickers_list) > 0:\n",
    "    for ticker in tickers_list:\n",
    "\n",
    "        print(ticker)\n",
    "        if check_size():\n",
    "\n",
    "            #Генерируем датасет\n",
    "            print(\"Начало обработки нового тикера: \", ticker)\n",
    "            try:\n",
    "                main(ticker)\n",
    "            except:\n",
    "                print(\"Ошибка генерации датасета: \", ticker)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "else:\n",
    "    print(\"Список для обработки пуст\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cda888",
   "metadata": {},
   "source": [
    "# Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9976be",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    'task_id': task_id,\n",
    "    'status': 'done'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c88679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохранение результатов в файл\n",
    "# with open('results/data_gen.json', 'w') as f:\n",
    "#     json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1559e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        url = 'http://'+respos_url+'/api/v1/task/complied'\n",
    "        response = requests.post(url, json = result)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Запрос успешно отправлен:\")\n",
    "            break\n",
    "    except Exception as err:\n",
    "        print(\"Ошибка отправка запроса на API:\", err)\n",
    "    \n",
    "    #Делаем повторные попытки в случае ошибки\n",
    "    if count >= 5:\n",
    "        break\n",
    "        \n",
    "    count += 1    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
