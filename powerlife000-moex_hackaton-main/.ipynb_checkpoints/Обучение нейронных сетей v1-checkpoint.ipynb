{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53098d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1f9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4fecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers, initializers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from array import *\n",
    "import os.path\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, auc, accuracy_score, roc_auc_score,f1_score,log_loss,\\\n",
    "classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sys import argv #Module for receiving parameters from the command line\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fe6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e7388",
   "metadata": {},
   "source": [
    "# Загрузка параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66440ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params_from_config_file = True #Загрузка параметров из файла\n",
    "load_params_from_command_line = False #Загрузка параметров из командной строки\n",
    "args = None\n",
    "\n",
    "try:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    _ = parser.add_argument('--config_file', dest='config_file', action='store_true', help='Load config from file')\n",
    "    _ = parser.add_argument('--config_path', help='Path to config file: /app/cfg.json')\n",
    "    _ = parser.add_argument('--cmd_config', dest='cmd_config', action='store_true', help='Load config from cmd line')\n",
    "    _ = parser.add_argument('--task_id')\n",
    "    _ = parser.add_argument('--data_path')\n",
    "    _ = parser.add_argument('--scaler_path')\n",
    "    _ = parser.add_argument('--neural_path')\n",
    "    _ = parser.add_argument('--new_model_flag')\n",
    "    _ = parser.add_argument('--learning_rate')\n",
    "    _ = parser.add_argument('--epochs')\n",
    "    _ = parser.add_argument('--steps_per_epoch')\n",
    "    _ = parser.add_argument('--validation_steps')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if args.config_file:\n",
    "        load_params_from_config_file = True\n",
    "        load_params_from_command_line = False\n",
    "    \n",
    "    if args.cmd_config:\n",
    "            load_params_from_config_file = False\n",
    "            load_params_from_command_line = True\n",
    "except:\n",
    "    print(\"Ошибка парсинга параметров из командной строки\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da9212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_params_from_config_file:\n",
    "    #Если есть параметры командной строки\n",
    "    if args:\n",
    "        #Если указан путь к конфигу\n",
    "        if args.config_path:\n",
    "            with open(config_path, 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "        else:\n",
    "            with open('app/configs/1D/edu_neural.json', 'r', encoding='utf_8') as cfg:\n",
    "                temp_data=cfg.read()\n",
    "\n",
    "    # parse file`\n",
    "    config = json.loads(temp_data)\n",
    "    \n",
    "    task_id = str(config['task_id'])\n",
    "    #Путь для загрузки генерируемых данных\n",
    "    data_path = config['data_path'] #Путь должен быть без чёрточки в конце\n",
    "    #Путь для сохранения скалера\n",
    "    scaler_path = config['scaler_path'] #Путь должен быть без чёрточки в конце\n",
    "    #Путь для сохранения нейронных сетей\n",
    "    neural_path = config['neural_path'] #Путь должен быть без чёрточки в конце\n",
    "    #Флаг необходимости подготовки новой модели (False - дообучение существующей)\n",
    "    new_model_flag = bool(config['new_model_flag'])\n",
    "    learning_rate = config['learning_rate']\n",
    "    epochs = config['epochs']\n",
    "    steps_per_epoch = config['steps_per_epoch']\n",
    "    validation_steps = config['validation_steps']\n",
    "    \n",
    "if load_params_from_command_line:\n",
    "    task_id = str(args.task_id)\n",
    "    data_path = str(args.data_path)\n",
    "    scaler_path = str(args.scaler_path)\n",
    "    neural_path = str(args.neural_path) \n",
    "    new_model_flag = bool(args.new_model_flag)\n",
    "    learning_rate = float(args.learning_rate) \n",
    "    epochs = int(args.epochs) \n",
    "    steps_per_epoch = int(args.steps_per_epoch) \n",
    "    validation_steps = int(args.validation_steps) \n",
    "\n",
    "Y_shift = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59496df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type = 'current'# Тип нейросети в ансамбле\n",
    "period = '1d'\n",
    "\n",
    "dataset_type = 'num_logic'\n",
    "dataset_timeframe = '1d_1w'\n",
    "\n",
    "#data_type_flag = False;\n",
    "#data_type_flag = 'float16';\n",
    "data_type_flag = 'float32';\n",
    "#data_type_flag = 'float64';\n",
    "\n",
    "#Флаг необходимости масштабирования данных\n",
    "scale_flag = True\n",
    "\n",
    "\n",
    "\n",
    "#Флаг тестирования модели\n",
    "test_model_flag = False\n",
    "\n",
    "#Флаг необходимости сохранения модели\n",
    "save_model_flag = True\n",
    "\n",
    "dataset = dataset_type + '_' + dataset_timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7750e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранённый датасет отсутствует\n",
      "Импортируем данные\n",
      "Доля NaN данных в датасете train: Datetime     0.0\n",
      "Close        0.0\n",
      "Y            0.0\n",
      "Open:5m      0.0\n",
      "High:5m      0.0\n",
      "            ... \n",
      "gap_14:5m    0.0\n",
      "gap_16:5m    0.0\n",
      "gap_18:5m    0.0\n",
      "gap_20:5m    0.0\n",
      "gap_40:5m    0.0\n",
      "Length: 2662, dtype: float64\n",
      "Доля NaN данных в датасете test: Datetime     0.0\n",
      "Close        0.0\n",
      "Y            0.0\n",
      "Open:5m      0.0\n",
      "High:5m      0.0\n",
      "            ... \n",
      "gap_14:5m    0.0\n",
      "gap_16:5m    0.0\n",
      "gap_18:5m    0.0\n",
      "gap_20:5m    0.0\n",
      "gap_40:5m    0.0\n",
      "Length: 2662, dtype: float64\n",
      "Число факторов:  2662\n",
      "Подготавливаем обучающие, тестовые и предиктивные выборки\n",
      "Выбираем данные\n",
      "Масштабируем данные\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Сохранённый датасет отсутствует\")\n",
    "# Импортируем данные для обучения и тестирования\n",
    "print(\"Импортируем данные\")\n",
    "if data_type_flag == 'float16':\n",
    "    init_data_train = pd.read_csv('./'+data_path+'/'+dataset+'_train.csv', dtype = 'float16', sep = ',')\n",
    "elif data_type_flag == 'float32':\n",
    "    init_data_train = pd.read_csv('./'+data_path+'/'+dataset+'_train.csv', dtype = 'float32', sep = ',')\n",
    "else:\n",
    "    init_data_train = pd.read_csv('./'+data_path+'/'+dataset+'_train.csv', sep = ',')\n",
    "if data_type_flag == 'float16':\n",
    "    init_data_test = pd.read_csv('./'+data_path+'/'+dataset+'_test.csv', dtype = 'float16', sep = ',')\n",
    "elif data_type_flag == 'float32':\n",
    "    init_data_test = pd.read_csv('./'+data_path+'/'+dataset+'_test.csv', dtype = 'float32', sep = ',')\n",
    "else:\n",
    "    init_data_test = pd.read_csv('./app/data/'+dataset+'_test.csv', sep = ',')\n",
    "print(\"Доля NaN данных в датасете train:\", init_data_train.isna().sum() / init_data_train.shape[0]*100)\n",
    "print(\"Доля NaN данных в датасете test:\", init_data_test.isna().sum() / init_data_test.shape[0]*100)\n",
    "\n",
    "#Исключаем nan и inf\n",
    "init_data_train.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "init_data_test.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Устанавливаем размерность датасетов\n",
    "n_train = init_data_train.shape[0]\n",
    "p_train = init_data_train.shape[1]\n",
    "print(\"Число факторов: \", p_train)\n",
    "n_test = init_data_test.shape[0]\n",
    "p_test = init_data_test.shape[1]\n",
    "# Формируем данные в numpy-массив\n",
    "init_data_train = init_data_train.values\n",
    "init_data_test = init_data_test.values\n",
    "# Подготовка данных для обучения и тестирования (проверки)\n",
    "print(\"Подготавливаем обучающие, тестовые и предиктивные выборки\")\n",
    "train_start = 0\n",
    "train_end = n_train\n",
    "test_start = 0\n",
    "test_end = n_test\n",
    "data_train = init_data_train[np.arange(train_start, train_end), :]\n",
    "data_test = init_data_test[np.arange(test_start, test_end), :]\n",
    "#Выбор данных\n",
    "print(\"Выбираем данные\")\n",
    "trainX = data_train[:, 3:]\n",
    "trainY = data_train[:, 2]\n",
    "train_quotes_close = data_train[:, 1]\n",
    "testX = data_test[:, 3:]\n",
    "testY = data_test[:, 2]\n",
    "test_quotes_close = data_test[:, 1]\n",
    "# Масштабирование данных\n",
    "print(\"Масштабируем данные\")\n",
    "x_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "if scale_flag: \n",
    "    x_scaler.fit(trainX)\n",
    "    scaler_filename = './'+scaler_path+'/scaler_'+dataset+'.save'\n",
    "    joblib.dump(x_scaler, scaler_filename) \n",
    "#Изменяем размерность массива, для обеспечения возможности масштабирования Y\n",
    "trainY = trainY.reshape(-1, 1)\n",
    "testY = testY.reshape(-1, 1)\n",
    "train_quotes_close = train_quotes_close.reshape(-1, 1)\n",
    "test_quotes_close = test_quotes_close.reshape(-1, 1)\n",
    "if scale_flag:\n",
    "    #y_scaler.fit(trainY)\n",
    "    trainX = x_scaler.transform(trainX)\n",
    "    testX = x_scaler.transform(testX)\n",
    "#Изменяем размерность массива Х, для рекурентной нейросети\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f849b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число анализируемых факторов 2659\n",
      "Число анализируемых данных тренировочной выборки 147912\n",
      "Число анализируемых данных тестовой выборки 184\n"
     ]
    }
   ],
   "source": [
    "#Проверяем число анализируемых факторов\n",
    "print(\"Число анализируемых факторов\", trainX.shape[2])\n",
    "print(\"Число анализируемых данных тренировочной выборки\", trainX.shape[0])\n",
    "print(\"Число анализируемых данных тестовой выборки\", testX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fabaf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_to_png(graph):\n",
    "    buffer = io.BytesIO()\n",
    "    graph.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_png = buffer.getvalue()\n",
    "    buffer.close()\n",
    "    graphic = base64.b64encode(image_png)\n",
    "    graphic = graphic.decode('utf-8')\n",
    "    graph.close()\n",
    "\n",
    "    return graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b73e16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_count = trainX.shape[2]\n",
    "data_count = trainX.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b3cc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_train():\n",
    "    while True:\n",
    "        x = trainX\n",
    "        y = trainY\n",
    "        yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7766db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_test():\n",
    "    while True:\n",
    "        x = testX\n",
    "        y = testY\n",
    "        yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01bc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfdata_generator(x_datas, y_datas, is_training, batch_size=128):\n",
    "    '''Construct a data generator using `tf.Dataset`. '''\n",
    "\n",
    "    def map_fn(x_data, y_data):\n",
    "        '''Preprocess raw data to trainable input. '''\n",
    "        x = x_data \n",
    "        y = y_data\n",
    "        return x, y\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_datas, y_datas))\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(1000)  # depends on sample size\n",
    "        dataset = dataset.map(map_fn)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31295bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tfdata_generator(trainX, trainY,is_training=True)\n",
    "\n",
    "train_generator = data_train()\n",
    "valid_generator = data_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a9f862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666a397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3fdb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9300951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверяем существование нейронной сети\n",
    "file_path = './'+neural_path+'/ansamble_'+dataset+'_v1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b47c47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Тестируем нейронную сеть\n",
    "if (os.access(file_path, os.F_OK) == True) & (test_model_flag == True):\n",
    "    print(\"Тестируем нейронную сеть\")\n",
    "    #Загружаем нейронную сеть\n",
    "    print(\"Загружаем сеть\")\n",
    "    model = load_model('./'+neural_path+'/ansamble_'+dataset+'_v1.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "518bc1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Дообучаем нейроннуюю сеть\n",
    "if (os.access(file_path, os.F_OK) == True) & (test_model_flag == False) & (new_model_flag == False):\n",
    "    print(\"Дообучаем нейронную сеть\")\n",
    "    #Загружаем нейронную сеть\n",
    "    print(\"Загружаем сеть\")\n",
    "    model = load_model('./'+neural_path+'/ansamble_'+dataset+'_v1.h5');\n",
    "    \n",
    "    #Обучаем нейронную сеть\n",
    "    print(\"Обучаем нейронную сеть\")\n",
    "    his = model.fit(\n",
    "        training_set, \n",
    "        validation_data=valid_generator, \n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        validation_steps = validation_steps, \n",
    "        callbacks=[\n",
    "            #model_checkpoint_callback,\n",
    "            #es\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if save_model_flag == True:    \n",
    "        #Сохраняем нейронную сеть\n",
    "        print(\"Сохраняем нейронную сеть\")\n",
    "        model.save('./'+neural_path+'/ansamble_'+dataset+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d043e169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нейронная сеть Отсутствует\n",
      "Формируем модель нейросети\n",
      "Компилируем нейронную сеть\n",
      "Обучаем нейронную сеть\n",
      "Epoch 1/10\n",
      "128/128 [==============================] - 5s 33ms/step - loss: 9.3434 - accuracy: 0.3996 - val_loss: 9.2770 - val_accuracy: 0.4185\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 9.2017 - accuracy: 0.4182 - val_loss: 9.1636 - val_accuracy: 0.3859\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 9.0776 - accuracy: 0.4393 - val_loss: 9.0272 - val_accuracy: 0.4293\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 8.9476 - accuracy: 0.4580 - val_loss: 8.8979 - val_accuracy: 0.4674\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 8.8174 - accuracy: 0.4730 - val_loss: 8.7890 - val_accuracy: 0.4620\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 8.7155 - accuracy: 0.4666 - val_loss: 8.6410 - val_accuracy: 0.5217\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 8.6034 - accuracy: 0.4677 - val_loss: 8.5236 - val_accuracy: 0.5109\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 8.4863 - accuracy: 0.4683 - val_loss: 8.4344 - val_accuracy: 0.4837\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 8.3701 - accuracy: 0.4867 - val_loss: 8.3041 - val_accuracy: 0.5054\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 8.2723 - accuracy: 0.4591 - val_loss: 8.2007 - val_accuracy: 0.5054\n",
      "Сохраняем нейронную сеть\n"
     ]
    }
   ],
   "source": [
    "if ((os.access(file_path, os.F_OK) == False) | (new_model_flag == True)) & (test_model_flag == False) :\n",
    "    print(\"Нейронная сеть Отсутствует\")\n",
    "    # define and fit the final model\n",
    "    print(\"Формируем модель нейросети\")\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    \n",
    "    #num_logic start\n",
    "    if dataset_type == 'num_logic':\n",
    "        model.add(Dense(\n",
    "            1000,\n",
    "            activation='relu', \n",
    "            kernel_regularizer=regularizers.l2(0.001),\n",
    "            kernel_initializer='random_normal',\n",
    "            bias_initializer='zeros'\n",
    "        ))   \n",
    "        model.add(Dense(\n",
    "            500,\n",
    "            activation='relu', \n",
    "            kernel_regularizer=regularizers.l2(0.001),\n",
    "            kernel_initializer='random_normal',\n",
    "            bias_initializer='zeros'\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=250, \n",
    "            #125,\n",
    "            activation='tanh', \n",
    "            kernel_regularizer=regularizers.l2(0.001),\n",
    "            kernel_initializer='random_normal',\n",
    "            bias_initializer='zeros'\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=150, \n",
    "            #75,\n",
    "            activation='relu', \n",
    "            kernel_regularizer=regularizers.l2(0.001),\n",
    "            kernel_initializer='random_normal',\n",
    "            bias_initializer='zeros'\n",
    "        ))\n",
    "    #num_logic end\n",
    "\n",
    "    model.add(Dense(\n",
    "        3, \n",
    "        kernel_regularizer=regularizers.l2(0.01),\n",
    "        kernel_initializer='random_normal',\n",
    "        bias_initializer='zeros',\n",
    "        activation='softmax'\n",
    "    ))\n",
    "\n",
    "    opt =  keras.optimizers.Adam(clipvalue=1., clipnorm=1., learning_rate = learning_rate,amsgrad = True)\n",
    "    print(\"Компилируем нейронную сеть\")\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t   \n",
    "    \n",
    "    #Обучаем нейронную сеть\n",
    "    print(\"Обучаем нейронную сеть\")\n",
    "    his = model.fit(\n",
    "        training_set, \n",
    "        validation_data=valid_generator, \n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        validation_steps = validation_steps, \n",
    "        callbacks=[\n",
    "            #model_checkpoint_ca|llback,\n",
    "            #es\n",
    "            #tensorboard_callback\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if save_model_flag == True:    \n",
    "        #Сохраняем нейронную сеть\n",
    "        print(\"Сохраняем нейронную сеть\")\n",
    "        model.save('./'+neural_path+'/ansamble_'+dataset+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dfab0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказываем результат\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "4623/4623 [==============================] - 16s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "print(\"Предсказываем результат\")\n",
    "predict_testY = model.predict(testX, verbose = 1)\n",
    "predict_trainY = model.predict(trainX, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9deab656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразовываем выходные сигналы тренировочной выборки\n",
    "result_predict_trainY = []\n",
    "\n",
    "for predict in predict_trainY:\n",
    "    result_predict_trainY.append(np.argmax(predict))\n",
    "        \n",
    "result_predict_trainY = np.array(result_predict_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b012b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразовываем выходные сигналы тестовой выборки\n",
    "result_predict_testY = []\n",
    "\n",
    "for predict in predict_testY:\n",
    "    result_predict_testY.append(np.argmax(predict))\n",
    "        \n",
    "result_predict_testY = np.array(result_predict_testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52eeae9",
   "metadata": {},
   "source": [
    "# Расчёт трендов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3520816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adimin\\AppData\\Local\\Temp\\ipykernel_156940\\3911187962.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  train_trends_origin.insert(i,last_train_signal)\n"
     ]
    }
   ],
   "source": [
    "#Расчёт трендов для тренировочной выборки на основе сигналов по разметке\n",
    "last_train_signal = 2\n",
    "train_trends_origin = array('f', []) #Массив ожидаемых данных по тренду\n",
    "for i in range(trainY.shape[0]):\n",
    "    if trainY[i] != last_train_signal and (trainY[i] == 2 or trainY[i] == 0):\n",
    "        last_train_signal = trainY[i]\n",
    "    train_trends_origin.insert(i,last_train_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab2a9f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adimin\\AppData\\Local\\Temp\\ipykernel_156940\\3764835393.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  test_trends_origin.insert(i,last_test_signal)\n"
     ]
    }
   ],
   "source": [
    "#Расчёт трендов для тестовой выборки на основе расчётных сигналов\n",
    "last_test_signal = 2\n",
    "test_trends_origin = array('f', []) #Массив ожидаемых данных по тренду\n",
    "for i in range(testY.shape[0]):\n",
    "    if testY[i] != last_test_signal and (testY[i] == 2 or testY[i] == 0):\n",
    "        last_test_signal = testY[i]\n",
    "    test_trends_origin.insert(i,last_test_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "565436f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Расчёт трендов для тренировочной выборки на основе расчётных данных\n",
    "last_train_signal = 2\n",
    "train_trends_predict = array('f', []) #Массив ожидаемых данных по тренду\n",
    "for i in range(len(result_predict_trainY)):\n",
    "    if result_predict_trainY[i] != last_train_signal and (result_predict_trainY[i] == 2 or result_predict_trainY[i] == 0):\n",
    "        last_train_signal = result_predict_trainY[i]\n",
    "    train_trends_predict.insert(i,last_train_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb03d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Расчёт трендов для тестовой выборки на основе расчётных сигналов\n",
    "last_test_signal = 2\n",
    "test_trends_predict = array('f', []) #Массив ожидаемых данных по тренду\n",
    "for i in range(len(result_predict_testY)):\n",
    "    if result_predict_testY[i] != last_test_signal and (result_predict_testY[i] == 2 or result_predict_testY[i] == 0):\n",
    "        last_test_signal = result_predict_testY[i]\n",
    "    test_trends_predict.insert(i,last_test_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c75e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trends_origin = np.asarray(train_trends_origin).astype(int)\n",
    "test_trends_origin = np.asarray(test_trends_origin).astype(int)\n",
    "train_trends_predict = np.asarray(train_trends_predict).astype(int)\n",
    "test_trends_predict = np.asarray(test_trends_predict).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956abd7",
   "metadata": {},
   "source": [
    "# Расчёт показателей точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcd23007",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_score = accuracy_score(train_trends_origin, train_trends_predict)\n",
    "train_roc_auc_score = roc_auc_score(train_trends_origin, train_trends_predict)\n",
    "train_precision_score = precision_score(train_trends_origin, train_trends_predict, pos_label=2)\n",
    "train_recall_score = recall_score(train_trends_origin, train_trends_predict, pos_label=2)\n",
    "train_f1_score = f1_score(train_trends_origin, train_trends_predict, pos_label=2)\n",
    "train_log_loss = log_loss(train_trends_origin, train_trends_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1807abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "РЕЗУЛЬТАТЫ АНАЛИЗА ТОЧНОСТИ\n",
      "ТРЕНИРОВОЧНАЯ ВЫБОРКА\n",
      "accuracy: 0.5578384444805019\n",
      "roc-auc: 0.5561395792438145\n",
      "precision: 0.5827536781754563\n",
      "recall: 0.3774179897069062\n",
      "f1: 0.4581299971001284\n",
      "logloss: 15.271826106297556\n"
     ]
    }
   ],
   "source": [
    "#Выводим данные результатов анализа точности\n",
    "print(\"РЕЗУЛЬТАТЫ АНАЛИЗА ТОЧНОСТИ\");\n",
    "\n",
    "print(\"ТРЕНИРОВОЧНАЯ ВЫБОРКА\")\n",
    "print('accuracy:', accuracy_score(train_trends_origin, train_trends_predict))\n",
    "print('roc-auc:', roc_auc_score(train_trends_origin, train_trends_predict))\n",
    "print('precision:', precision_score(train_trends_origin, train_trends_predict, pos_label=2))\n",
    "print('recall:', recall_score(train_trends_origin, train_trends_predict, pos_label=2))\n",
    "print('f1:', f1_score(train_trends_origin, train_trends_predict, pos_label=2))\n",
    "print('logloss:', log_loss(train_trends_origin, train_trends_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "898e42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_score = accuracy_score(test_trends_origin, test_trends_predict)\n",
    "test_roc_auc_score = roc_auc_score(test_trends_origin, test_trends_predict)\n",
    "test_precision_score = precision_score(test_trends_origin, test_trends_predict, pos_label=2)\n",
    "test_recall_score = recall_score(test_trends_origin, test_trends_predict, pos_label=2)\n",
    "test_f1_score = f1_score(test_trends_origin, test_trends_predict, pos_label=2)\n",
    "test_log_loss = log_loss(test_trends_origin, test_trends_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca6551fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕСТОВАЯ ВЫБОРКА\n",
      "accuracy: 0.44565217391304346\n",
      "roc-auc: 0.4381486341405225\n",
      "precision: 0.379746835443038\n",
      "recall: 0.3614457831325301\n",
      "f1: 0.37037037037037035\n",
      "logloss: 19.1467085464944\n"
     ]
    }
   ],
   "source": [
    "print(\"ТЕСТОВАЯ ВЫБОРКА\")\n",
    "print('accuracy:', accuracy_score(test_trends_origin, test_trends_predict))\n",
    "print('roc-auc:', roc_auc_score(test_trends_origin, test_trends_predict))\n",
    "print('precision:', precision_score(test_trends_origin, test_trends_predict, pos_label=2))\n",
    "print('recall:', recall_score(test_trends_origin, test_trends_predict, pos_label=2))\n",
    "print('f1:', f1_score(test_trends_origin, test_trends_predict, pos_label=2))\n",
    "print('logloss:', log_loss(test_trends_origin, test_trends_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714aa4d9",
   "metadata": {},
   "source": [
    "# Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0713b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    'task_id': task_id,\n",
    "    'edu_graph_losses': {\n",
    "        'loss': {\n",
    "            'description': 'Loss тренировочной выборки',\n",
    "            'values': his.history['loss']\n",
    "        },\n",
    "        'val_loss': {\n",
    "            'description': 'Loss валидационной выборки',\n",
    "            'values': his.history['val_loss']\n",
    "        }\n",
    "    },\n",
    "    'train_accuracy_score': {\n",
    "        'description': 'Метрика точности accuracy тренировочной выборки',\n",
    "        'values': train_accuracy_score\n",
    "    }, \n",
    "    'train_roc_auc_score': {\n",
    "        'description': 'Метрика точности roc_auc тренировочной выборки',\n",
    "        'values': train_roc_auc_score\n",
    "    }, \n",
    "    'train_precision_score': {\n",
    "        'description': 'Метрика точности precision тренировочной выборки',\n",
    "        'values': train_precision_score\n",
    "    }, \n",
    "    'train_recall_score': {\n",
    "        'description': 'Метрика точности recall тренировочной выборки',\n",
    "        'values': train_recall_score\n",
    "    }, \n",
    "    'train_f1_score': {\n",
    "        'description': 'Метрика точности f1 тренировочной выборки',\n",
    "        'values': train_f1_score\n",
    "    }, \n",
    "    'train_log_loss': {\n",
    "        'description': 'Метрика точности log_loss тренировочной выборки',\n",
    "        'values': train_log_loss\n",
    "    },\n",
    "    'test_accuracy_score': {\n",
    "        'description': 'Метрика точности accuracy тестовой выборки',\n",
    "        'values': test_accuracy_score\n",
    "    }, \n",
    "    'test_roc_auc_score': {\n",
    "        'description': 'Метрика точности roc_auc тестовой выборки',\n",
    "        'values': test_roc_auc_score\n",
    "    }, \n",
    "    'test_precision_score': {\n",
    "        'description': 'Метрика точности precision тестовой выборки',\n",
    "        'values': test_precision_score\n",
    "    }, \n",
    "    'test_recall_score': {\n",
    "        'description': 'Метрика точности recall тестовой выборки',\n",
    "        'values': test_recall_score\n",
    "    }, \n",
    "    'test_f1_score': {\n",
    "        'description': 'Метрика точности f1 тестовой выборки',\n",
    "        'values': test_f1_score\n",
    "    }, \n",
    "    'test_log_loss': {\n",
    "        'description': 'Метрика точности log тестовой выборки',\n",
    "        'values': test_log_loss\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4350ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/edu_neurals.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e1377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
